{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import requests #to unshorten url\n",
    "import regex_expr #for url, hashtag and mentions\n",
    "import re\n",
    "import tldextract #to get hashtag domain\n",
    "from langdetect import detect #to filter English tweets\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from requests.exceptions import ConnectionError, MissingSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading - Social Honeypot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_legitimate_df = pd.read_csv( filepath_or_buffer = 'data\\social_honeypot\\legitimate_users.txt', \n",
    "                                     sep    ='\\t', \n",
    "                                     header = None,\n",
    "                                     names  = ['UserID', \n",
    "                                              'CreatedAt',\n",
    "                                              'CollectedAt',\n",
    "                                              'NumberOfFollowings',\n",
    "                                              'NumberOfFollowers',\n",
    "                                              'NumberOfTweets',\n",
    "                                              'LengthOfScreenName',\n",
    "                                              'LengthOfDescriptionInUserProfile'])\n",
    "\n",
    "tweet_legitimate_df = pd.read_csv(   filepath_or_buffer = 'data/social_honeypot/legitimate_users_tweets.txt', \n",
    "                                 sep    ='\\t', \n",
    "                                 header = None,\n",
    "                                 names  = ['UserID', \n",
    "                                          'Tweet_ID',\n",
    "                                          'Tweet',\n",
    "                                          'CreatedAt'])\n",
    "\n",
    "\n",
    "\n",
    "tweet_polluters_df = pd.read_csv(filepath_or_buffer = 'data/social_honeypot/content_polluters_tweets.txt', \n",
    "                                 sep    ='\\t', \n",
    "                                 header = None,\n",
    "                                 names  = ['UserID', \n",
    "                                          'Tweet_ID',\n",
    "                                          'Tweet',\n",
    "                                          'CreatedAt'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There are 20645 spammers and 19251 legitimate users in the dataset:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20602"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_polluters_df.UserID.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19208"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_legitimate_df.UserID.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Turns out some some User IDs present in both datasets - those must be excluded\"\"\"\n",
    "ambiguous_users     = np.intersect1d(tweet_legitimate_df.UserID.unique(), tweet_polluters_df.UserID.unique())\n",
    "tweet_polluters_df  = tweet_polluters_df[~tweet_polluters_df.UserID.isin(ambiguous_users)]\n",
    "tweet_legitimate_df = tweet_legitimate_df[~tweet_legitimate_df.UserID.isin(ambiguous_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We are going to use a part of dataset to build the algorithm. Let's assume having 1/3 of the users from each cohort.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Randomly select 1/3 of users from each cohort\"\"\"\n",
    "\n",
    "sample_rate = 1/3\n",
    "\n",
    "#legitimate users\n",
    "sample_size_leg     = int(tweet_legitimate_df['UserID'].unique().shape[0] * sample_rate)\n",
    "legitimate_sample   = np.random.choice(tweet_legitimate_df['UserID'].unique(), \n",
    "                                       size    =sample_size_leg, \n",
    "                                       replace =False)\n",
    "tweet_leg_df_sample = tweet_legitimate_df[tweet_legitimate_df['UserID'].isin(legitimate_sample)]\n",
    "\n",
    "\n",
    "#spam users\n",
    "sample_size_spam    = int(tweet_polluters_df['UserID'].unique().shape[0] * sample_rate)\n",
    "polluters_sample    = np.random.choice(tweet_polluters_df['UserID'].unique(), \n",
    "                                       size    =sample_size_spam, \n",
    "                                       replace =False)\n",
    "tweet_pol_df_sample = tweet_polluters_df[tweet_polluters_df['UserID'].isin(polluters_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"REMOVE NULL VALUES FROM TWEETS\"\"\"\n",
    "tweet_pol_df_sample = tweet_pol_df_sample[~ pd.isnull(tweet_pol_df_sample['Tweet'])]\n",
    "tweet_leg_df_sample = tweet_leg_df_sample[~ pd.isnull(tweet_leg_df_sample['Tweet'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(string):\n",
    "    try:\n",
    "        return detect(string)\n",
    "    except LangDetectException:\n",
    "        return(\"Language could not be detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine language of tweets\n",
    "#tweet_leg_df_sample['Tweet_lang'] = tweet_leg_df_sample['Tweet'].apply(lambda x: detect_lang(x))\n",
    "#tweet_pol_df_sample['Tweet_lang'] = tweet_pol_df_sample['Tweet'].apply(lambda x: detect_lang(x))\n",
    "\n",
    "\n",
    "#SAVING RAW DATA WITH LANGUAGE AND CLASS LABELS TO CSV\n",
    "tweet_leg_df_sample.to_csv(\"data\\samples\\legitimate_users.csv\", index= False, sep = '|')\n",
    "tweet_pol_df_sample.to_csv(\"data\\samples\\content_polluters.csv\", index= False, sep = '|')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
>>>>>>> 6e13df548555540dc87a921d0afdfd3ec2597168
    "tweet_leg_df_sample = pd.read_csv(\"data\\samples\\legitimate_users.csv\", sep = '|')\n",
    "tweet_pol_df_sample = pd.read_csv(\"data\\samples\\content_polluters.csv\", sep = '|')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 7,
>>>>>>> 6e13df548555540dc87a921d0afdfd3ec2597168
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6867"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 7,
>>>>>>> 6e13df548555540dc87a921d0afdfd3ec2597168
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_pol_df_sample.UserID.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6867"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_pol_df_sample.UserID.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_leg_df_sample.Tweet_ID.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5854"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_leg_df_sample.UserID.unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on samples of legitimate and spam accounts\n",
    "All languages:\n",
    "- ~778179 tweets from spam accounts\n",
    "- ~572536 tweets from legitimate accounts\n",
    "\n",
    "Only English:\n",
    "- ~ 43291 tweets\n",
    "- ~ 43444 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "\"legitimate user tweets\"\n",
    "tweet_leg_df_sample = tweet_leg_df_sample[tweet_leg_df_sample['Tweet_lang'] == 'en']\n",
    "\n",
    "#drop useless columns\n",
    "tweet_leg_df_sample.drop(columns = ['Tweet_lang'], inplace=True)\n",
    "\n",
    "\"polluting user tweets\"\n",
    "tweet_pol_df_sample = tweet_pol_df_sample[tweet_pol_df_sample['Tweet_lang'] == 'en']\n",
    "\n",
    "#drop useless columns\n",
    "tweet_pol_df_sample.drop(columns = ['Tweet_lang', 'CreatedAt'], inplace=True)\n",
    "\n",
    "#Stack dataframes into one\n",
    "tweet_df = pd.concat([tweet_leg_df_sample, tweet_pol_df_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting URL, Hashtags and Mentions from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FULL URL EXTRACTION\"\"\"\n",
    "###############################################\n",
    "\n",
    "\"\"\"\n",
    "    regex for URL pattern matching has been taken from https://gist.github.com/gruber/8891611\n",
    "\"\"\"\n",
    "#find URLs\n",
    "tweet_df['URL'] = tweet_df['Tweet'].apply(lambda s: re.findall(regex_expr.URL, s))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"HASHTAG EXTRACTION\"\"\"\n",
    "###############################################\n",
    "\n",
    "\"\"\"\n",
    "    regex for HASHTAG pattern matching has been taken from https://gist.github.com/mahmoud/237eb20108b5805aed5f\n",
    "\"\"\"\n",
    "#find HASHTAGS\n",
    "tweet_df['Hashtag'] = tweet_df['Tweet'].apply(lambda s: re.findall(regex_expr.HASHTAG, s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MENTIONS EXTRACTION\"\"\"\n",
    "###############################################\n",
    "\n",
    "\"\"\"\n",
    "    regex for MENTION pattern matching has been taken from https://gist.github.com/mahmoud/237eb20108b5805aed5f\n",
    "\"\"\"\n",
    "#find mentions\n",
    "tweet_df['Mention'] = tweet_df['Tweet'].apply(lambda s: re.findall(regex_expr.MENTION, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SAVING PREPROCESSED SAMPLE\"\"\"\n",
    "tweet_df.to_csv('tweets_sample_preprocessed.csv', index= False, sep = '|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
