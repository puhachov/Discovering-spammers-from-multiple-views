{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import re\n",
    "import numpy as np\n",
    "import multiview as mv\n",
    "import singleview as sv\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import ast #convert string list to list\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('data/tweets_sample_preprocessed.zip',compression = 'zip', sep = '|')\n",
    "tweet_df = tweet_df[tweet_df.UserID != 84165878]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Secondary functions\"\"\"\n",
    "def count_phrase_freq(phrase, text):\n",
    "    phrase = phrase.lower()\n",
    "    text = text.lower()\n",
    "    regex_obj = re.findall('\\\\b'+phrase+'\\\\b', text)\n",
    "    if regex_obj:\n",
    "        return len(regex_obj)\n",
    "    else:\n",
    "        return 0\n",
    "spam_list = [line.rstrip('\\n') for line in open('spam_phrases.txt', 'r')]\n",
    "\n",
    "def count_spam_phrases_per_tweet(spam_list, tweet):    \n",
    "    count = 0\n",
    "    for phrase in spam_list:\n",
    "        count += count_phrase_freq(phrase, tweet)\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content-based features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature: num of mentions in tweet\n",
    "tweet_df['NumOfMentions'] = tweet_df['Mention'].map(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "\n",
    "def retweet_rate(tweet_df):\n",
    "    tweet_df['hasRetweet'] = tweet_df.Tweet.str.contains(\"^RE \")\n",
    "    num_tweets_with_RT     = tweet_df.groupby('UserID')['hasRetweet'].sum()\n",
    "    total_num_tweets       = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    feature                = num_tweets_with_RT/total_num_tweets\n",
    "    tweet_df.drop(columns='hasRetweet')\n",
    "    return feature\n",
    "\n",
    "\n",
    "def avg_length_of_tweet(tweet_df):\n",
    "    tweet_df['Tweet_Length'] = tweet_df['Tweet'].str.len()\n",
    "    tweet_length             = tweet_df.groupby('UserID')['Tweet_Length'].sum()\n",
    "    num_of_tweets            = tweet_df.groupby('UserID')['Tweet_Length'].count()\n",
    "    feature                  = tweet_length/num_of_tweets\n",
    "    tweet_df.drop(columns='Tweet_Length', inplace=True)\n",
    "    return feature\n",
    "\n",
    "def avg_num_mentions_per_tweet(tweet_df):\n",
    "    \n",
    "    num_mentions_per_user = tweet_df.groupby('UserID')['NumOfMentions'].sum()\n",
    "    num_tweets_per_user   = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    feature               = num_mentions_per_user/num_tweets_per_user\n",
    "    return feature\n",
    "\n",
    "#count spam phrases in tweets, source: (https://blog.hubspot.com/blog/tabid/6307/bid/30684/the-ultimate-list-of-email-spam-trigger-words.aspx)\n",
    "      \n",
    "def avg_num_spam_phrases_per_tweet(tweet_df):\n",
    "    tweet_df['NumSpamWords']  = list(map(lambda x: count_spam_phrases_per_tweet(spam_list, x), tweet_df.Tweet))\n",
    "    sum_spam_phrases_per_user = tweet_df.groupby('UserID')['NumSpamWords'].sum()\n",
    "    num_tweets_per_user       = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    feature                   = sum_spam_phrases_per_user/num_tweets_per_user\n",
    "    return feature\n",
    "    \n",
    "#tweet_df.drop(columns='NumOfMentions', inplace=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature: num of hashtags in tweet\n",
    "tweet_df['NumOfHashtags'] = tweet_df.Hashtag.map(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "#average number of Hashtags per tweet\n",
    "def avg_num_hashtags(tweet_df):\n",
    "    count_URL_per_user    = tweet_df.groupby('UserID')['NumOfHashtags'].sum()\n",
    "    count_Tweets_per_user = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    return count_URL_per_user/count_Tweets_per_user\n",
    "\n",
    "#\n",
    "def avg_same_hashtag_count(tweet_df):\n",
    "    tweet_df['isHashtagUnique']    = np.where(tweet_df['NumOfHashtags'] == 1, 1, 0)\n",
    "    tweet_df['isHashtagDuplicate'] = np.where(tweet_df['NumOfHashtags'] > 1, 1, 0)\n",
    "    num_unique_hashtags            = tweet_df.groupby('UserID')['isHashtagUnique'].sum()\n",
    "    num_duplicate_hashtags         = tweet_df.groupby('UserID')['isHashtagDuplicate'].sum()\n",
    "    total_tweet_count              = num_duplicate_hashtags = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    feature = num_duplicate_hashtags/(num_unique_hashtags*total_tweet_count)\n",
    "    feature = feature.replace(np.inf, 0)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def num_hashtags_per_tweet(tweet_df):\n",
    "    tweet_df['hasHashtag']     = tweet_df[tweet_df['NumOfHashtags'] > 0]\n",
    "    total_tweet_count          = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    num_tweets_with_hashtag    = tweet_df.groupby('UserID')['hasHashtag'].sum()\n",
    "    feature = num_tweets_with_hashtag/total_tweet_count\n",
    "    return feature\n",
    "    \n",
    "\n",
    "\n",
    "#tweet_df.drop(columns='NumOf#', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature: num of mentions in tweet\n",
    "tweet_df['NumOfURLs'] = tweet_df['URL'].map(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "\n",
    "#average number of URLs per tweet\n",
    "def avg_num_URLs(tweet_df):\n",
    "    count_URL_per_user    = tweet_df.groupby('UserID')['NumOfURLs'].sum()\n",
    "    count_Tweets_per_user = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    return count_URL_per_user/count_Tweets_per_user\n",
    "\n",
    "def avg_same_URL_count(tweet_df):\n",
    "    tweet_df['isURLUnique']    = np.where(tweet_df['NumOfURLs'] == 1, 1, 0)\n",
    "    tweet_df['isURLDuplicate'] = np.where(tweet_df['NumOfURLs'] > 1, 1, 0)\n",
    "    num_unique_URLs            = tweet_df.groupby('UserID')['isURLUnique'].sum()\n",
    "    num_duplicate_URLs         = tweet_df.groupby('UserID')['isURLDuplicate'].sum()\n",
    "    total_tweet_count          = num_duplicate_URLs = tweet_df.groupby('UserID').Tweet.count()\n",
    "    feature = num_duplicate_URLs/(num_unique_URLs*total_tweet_count)\n",
    "    feature = feature.replace(np.inf, 0)\n",
    "    return feature\n",
    "\n",
    "\n",
    "\n",
    "#tweet_df.drop(columns='NumOfURLs#', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining features into a single-view matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    content_view_df = pd.read_csv(r'data/views_df_preprocessed/content_view_df.csv', sep = '|', index_col=0)\n",
    "    URL_view_df     = pd.read_csv(r'data/views_df_preprocessed/URL_view_df.csv', sep = '|', index_col=0)\n",
    "    hashtag_view_df = pd.read_csv(r'data/views_df_preprocessed/hashtag_view_df.csv', sep = '|', index_col=0)\n",
    "except:\n",
    "    #Content-based view\n",
    "    content_view_df = pd.DataFrame(dict(AvgLengthOfTweets = avg_length_of_tweet(tweet_df), \n",
    "                                        RetweetRate       = retweet_rate(tweet_df),\n",
    "                                        AvgNumMentions    = avg_num_mentions_per_tweet(tweet_df),\n",
    "                                        AvgNumSpamPhrases = avg_num_spam_phrases_per_tweet(tweet_df)                                    \n",
    "                                       ))\n",
    "\n",
    "    #URL-based view\n",
    "    URL_view_df = pd.DataFrame(dict(AvgNumURLs            = avg_num_URLs(tweet_df),\n",
    "                                    AvgSameURLCount       = avg_same_URL_count(tweet_df)))\n",
    "\n",
    "    #Hashtag-based view\n",
    "    hashtag_view_df = pd.DataFrame(dict(AvgNumHashtags = avg_num_hashtags(tweet_df),\n",
    "                                        AvgSamHashtagCount   = avg_same_hashtag_count(tweet_df)\n",
    "                                       ))\n",
    "    \n",
    "    content_view_df.to_csv(r\"data\\views_df_preprocessed\\content_view_df.csv\", index= True, sep = '|')\n",
    "    URL_view_df.to_csv(r\"data\\views_df_preprocessed\\URL_view_df.csv\", index= True, sep = '|')\n",
    "    hashtag_view_df.to_csv(r\"data\\views_df_preprocessed\\hashtag_view_df.csv\", index= True, sep = '|')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating label matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_legitimate_df = pd.read_csv('data\\social_honeypot\\legitimate_users.txt', \n",
    "                                  sep = '\\t',\n",
    "                                  names = ['UserID',\n",
    "                                           'CreatedAt',\n",
    "                                           'CollectedAt',\n",
    "                                           'NumberOfFollowings',\n",
    "                                           'NumberOfFollowers',\n",
    "                                           'NumberOfTweets',\n",
    "                                           'LengthOfScreenName',\n",
    "                                           'LengthOfDescriptionInUserPro'])\n",
    "users_polluters_df = pd.read_csv('data/social_honeypot/content_polluters.txt', \n",
    "                                  sep = '\\t',\n",
    "                                  names = ['UserID',\n",
    "                                           'CreatedAt',\n",
    "                                           'CollectedAt',\n",
    "                                           'NumberOfFollowings',\n",
    "                                           'NumberOfFollowers',\n",
    "                                           'NumberOfTweets',\n",
    "                                           'LengthOfScreenName',\n",
    "                                           'LengthOfDescriptionInUserPro'])\n",
    "tweet_df['isSpammer']    = np.where(tweet_df['UserID'].isin(list(users_polluters_df['UserID'])), -1, 0)\n",
    "tweet_df['isLegitimate'] = np.where(tweet_df['UserID'].isin(list(users_legitimate_df['UserID'])), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "class_label_df = tweet_df[['UserID','isLegitimate', 'isSpammer']].drop_duplicates(['UserID']).sort_values('UserID').set_index('UserID')\n",
    "class_label_df = class_label_df[['isSpammer','isLegitimate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiview Spam Detection Algorithm (MVSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mv)\n",
    "#content_view_df.AvgLengthOfTweets = content_view_df.AvgLengthOfTweets/content_view_df.AvgLengthOfTweets.max()\n",
    "X_nv = [content_view_df, URL_view_df, hashtag_view_df]\n",
    "\n",
    "#shuffle data points\n",
    "X_nv = [df.sample(frac = 1, random_state = 2) for df in X_nv]\n",
    "\n",
    "# normalize X\n",
    "X_nv = [normalize(X, axis = 0,  norm = 'l1') for X in X_nv]\n",
    "\n",
    "#transpose to correspond to the notations of dimensions used in the paper\n",
    "X_nv = [np.transpose(X_nv[v]) for v in range(len(X_nv))]\n",
    "\n",
    "Y = np.array(class_label_df.sample(frac = 1, random_state = 2))\n",
    "mvsd = mv.multiview(X = X_nv, Y = Y, num_components = 10 )\n",
    "mvsd.solve(training_size=0.70, learning_rate= 0.001, alpha=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, precision, recall, F1_score = mvsd.evaluate_train()\n",
    "confusion_matrix_ = pd.DataFrame(data = {'Actual_Spammer': confusion_matrix[:,0], 'Actual_Legitimate': confusion_matrix[:,1]}, index = ['Predicted_Spammer ','Predicted_Legitimate'])\n",
    "print(confusion_matrix_)\n",
    "print(\"\\n\")\n",
    "print(\"Precision: {}\\n\".format(precision))\n",
    "print(\"Recall: {}\\n\".format(recall))\n",
    "print(\"F1-score: {}\\n\".format(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, precision, recall, F1_score = mvsd.evaluate_test()\n",
    "confusion_matrix_ = pd.DataFrame(data = {'Actual_Spammer': confusion_matrix[:,0], 'Actual_Legitimate': confusion_matrix[:,1]}, index = ['Predicted_Spammer ','Predicted_Legitimate'])\n",
    "print(confusion_matrix_)\n",
    "print(\"\\n\")\n",
    "print(\"Precision: {}\\n\".format(precision))\n",
    "print(\"Recall: {}\\n\".format(recall))\n",
    "print(\"F1-score: {}\\n\".format(F1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with single-view approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content view features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sv)\n",
    "X_nv = [content_view_df, URL_view_df, hashtag_view_df]\n",
    "X_nv = [df.sample(frac = 1, random_state = 2) for df in X_nv]\n",
    "X_nv = [np.transpose(X_nv[v]) for v in range(len(X_nv))]\n",
    "Y = np.array(class_label_df.sample(frac = 1, random_state = 2))\n",
    "\n",
    "content_view_svm = sv.singleview(data = X_nv[0],  class_ = Y)\n",
    "model_svm  = SVC(gamma = \"auto\")\n",
    "training_sizes = [0.30, 0.50, 0.80]\n",
    "for s in training_sizes:\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "    print(\"Training size: {}\\n\".format(s))\n",
    "    precision, recall, F1_score, confusion_matrix_CV = content_view_svm.evaluate(model = model_svm, training_size=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sv)\n",
    "X_nv = [content_view_df, URL_view_df, hashtag_view_df]\n",
    "X_nv = [df.sample(frac = 1, random_state = 2) for df in X_nv]\n",
    "X_nv = [np.transpose(X_nv[v]) for v in range(len(X_nv))]\n",
    "Y = np.array(class_label_df.sample(frac = 1, random_state = 2))\n",
    "\n",
    "content_view_svm = sv.singleview(data = X_nv[1],  class_ = Y)\n",
    "model_svm  = SVC(gamma = \"auto\")\n",
    "training_sizes = [0.30, 0.50, 0.80]\n",
    "for s in training_sizes:\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "    print(\"Training size: {}\\n\".format(s))\n",
    "    precision, recall, F1_score, confusion_matrix_CV = content_view_svm.evaluate(model = model_svm, training_size=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sv)\n",
    "X_nv = [content_view_df, URL_view_df, hashtag_view_df]\n",
    "X_nv = [df.sample(frac = 1, random_state = 2) for df in X_nv]\n",
    "X_nv = [np.transpose(X_nv[v]) for v in range(len(X_nv))]\n",
    "Y = np.array(class_label_df.sample(frac = 1, random_state = 2))\n",
    "\n",
    "content_view_svm = sv.singleview(data = X_nv[2],  class_ = Y)\n",
    "model_svm  = SVC(gamma = \"auto\")\n",
    "training_sizes = [0.30, 0.50, 0.80]\n",
    "for s in training_sizes:\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "    print(\"Training size: {}\\n\".format(s))\n",
    "    precision, recall, F1_score, confusion_matrix_CV = content_view_svm.evaluate(model = model_svm, training_size=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sv)\n",
    "\n",
    "Y = np.array(class_label_df.sample(frac = 1, random_state = 2))\n",
    "X = np.array(pd.concat(X_nv, axis=0))\n",
    "content_view_svm = sv.singleview(data = X,  class_ = Y)\n",
    "model_svm  = SVC(gamma = \"auto\")\n",
    "training_sizes = [0.30, 0.50, 0.80]\n",
    "for s in training_sizes:\n",
    "    print(\"---------------------------------------------------------------------\")\n",
    "    print(\"Training size: {}\\n\".format(s))\n",
    "    precision, recall, F1_score, confusion_matrix_CV = content_view_svm.evaluate(model = model_svm, training_size=s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
