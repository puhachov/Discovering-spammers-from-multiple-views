{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import linalg, optimize\n",
    "from numpy.linalg import multi_dot\n",
    "import pandas as pd\n",
    "import ast #convert string list to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('tweets_sample_preprocessed.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content-based features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature: num of mentions in tweet\n",
    "tweet_df['Mention'].astype('object')\n",
    "tweet_df['NumOfMentions'] = tweet_df['Mention'].map(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "\n",
    "def retweet_rate(tweet_df):\n",
    "    tweet_df['hasRetweet'] = tweet_df.Tweet.str.contains(\"^RE \")\n",
    "    num_tweets_with_RT     = tweet_df.groupby('UserID')['hasRetweet'].sum()\n",
    "    total_num_tweets       = tweet_df.groupby('UserID')['hasRetweet'].count()\n",
    "    feature                = num_tweets_with_RT/total_num_tweets\n",
    "    tweet_df.drop(columns='hasRetweet')\n",
    "    return feature\n",
    "\n",
    "\n",
    "def avg_length_of_tweet(tweet_df):\n",
    "    tweet_df['Tweet_Length'] = tweet_df['Tweet'].str.len()\n",
    "    tweet_length             = tweet_df.groupby('UserID')['Tweet_Length'].sum()\n",
    "    num_of_tweets            = tweet_df.groupby('UserID')['Tweet_Length'].count()\n",
    "    feature                  = tweet_length/num_of_tweets\n",
    "    tweet_df.drop(columns='Tweet_Length', inplace=True)\n",
    "    return feature\n",
    "\n",
    "def avg_num_mentions_per_tweet(tweet_df):\n",
    "    \n",
    "    num_mentions_per_user = tweet_df.groupby('UserID')['NumOfMentions'].count()\n",
    "    num_tweets_per_user   = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    feature               = num_mentions_per_user/num_tweets_per_user\n",
    "    return feature\n",
    "\n",
    "#tweet_df.drop(columns='NumOfMentions', inplace=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature: num of hashtags in tweet\n",
    "tweet_df['NumOfHashtags'] = tweet_df.Hashtag.map(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "#average number of Hashtags per tweet\n",
    "def avg_num_hashtags(tweet_df):\n",
    "    count_URL_per_user    = tweet_df.groupby('UserID')['NumOfHashtags'].sum()\n",
    "    count_Tweets_per_user = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    return count_URL_per_user/count_Tweets_per_user\n",
    "\n",
    "#\n",
    "def avg_same_hashtag_count(tweet_df):\n",
    "    tweet_df['isHashtagUnique']    = np.where(tweet_df['NumOfHashtags'] == 1, 1, 0)\n",
    "    tweet_df['isHashtagDuplicate'] = np.where(tweet_df['NumOfHashtags'] > 1, 1, 0)\n",
    "    num_unique_hashtags            = tweet_df.groupby('UserID')['isHashtagUnique'].sum()\n",
    "    num_duplicate_hashtags         = tweet_df.groupby('UserID')['isHashtagDuplicate'].sum()\n",
    "    total_tweet_count              = num_duplicate_hashtags = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    feature = num_duplicate_hashtags/(num_unique_hashtags*total_tweet_count)\n",
    "    feature = feature.replace(np.inf, 0)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def num_hashtags_per_tweet(tweet_df):\n",
    "    tweet_df['hasHashtag']     = tweet_df[tweet_df['NumOfHashtags'] > 0]\n",
    "    total_tweet_count          = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    num_tweets_with_hashtag    = tweet_df.groupby('UserID')['hasHashtag'].sum()\n",
    "    feature = num_tweets_with_hashtag/total_tweet_count\n",
    "    return feature\n",
    "    \n",
    "\n",
    "\n",
    "#tweet_df.drop(columns='NumOf#', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature: num of mentions in tweet\n",
    "tweet_df['NumOfURLs'] = tweet_df['URL'].map(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "\n",
    "#average number of URLs per tweet\n",
    "def avg_num_URLs(tweet_df):\n",
    "    count_URL_per_user    = tweet_df.groupby('UserID')['NumOfURLs'].sum()\n",
    "    count_Tweets_per_user = tweet_df.groupby('UserID')['Tweet'].count()\n",
    "    return count_URL_per_user/count_Tweets_per_user\n",
    "\n",
    "def avg_same_URL_count(tweet_df):\n",
    "    tweet_df['isURLUnique']    = np.where(tweet_df['NumOfURLs'] == 1, 1, 0)\n",
    "    tweet_df['isURLDuplicate'] = np.where(tweet_df['NumOfURLs'] > 1, 1, 0)\n",
    "    num_unique_URLs            = tweet_df.groupby('UserID')['isURLUnique'].sum()\n",
    "    num_duplicate_URLs         = tweet_df.groupby('UserID')['isURLDuplicate'].sum()\n",
    "    total_tweet_count          = num_duplicate_URLs = tweet_df.groupby('UserID').Tweet.count()\n",
    "    feature = num_duplicate_URLs/(num_unique_URLs*total_tweet_count)\n",
    "    feature = feature.replace(np.inf, 0)\n",
    "    return feature\n",
    "\n",
    "\n",
    "\n",
    "#tweet_df.drop(columns='NumOfURLs#', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining features into a single-view matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Content-based view\n",
    "content_view_df = pd.DataFrame(dict(AvgLengthOfTweets = avg_length_of_tweet(tweet_df), \n",
    "                                    #RetweetRate       = retweet_rate(tweet_df),\n",
    "                                    AvgNumMentions    = avg_num_mentions_per_tweet(tweet_df)\n",
    "                                    \n",
    "                                   ))\n",
    "\n",
    "#URL-based view\n",
    "URL_view_df = pd.DataFrame(dict(AvgNumURLs            = avg_num_URLs(tweet_df),\n",
    "                                AvgSameURLCount       = avg_same_URL_count(tweet_df)))\n",
    "\n",
    "#Hashtag-based view\n",
    "hashtag_view_df = pd.DataFrame(dict(AvgNumHashtags = avg_num_hashtags(tweet_df),\n",
    "                                    AvgSamHashtagCount   = avg_same_hashtag_count(tweet_df)\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unshorten URLs and extract domains and suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_URL_domain(url, session):\n",
    "    if pd.isnull(url):\n",
    "        return ''\n",
    "    else:\n",
    "        try:            \n",
    "            url_response = session.head(url, allow_redirects = True).url\n",
    "            return tldextract.extract(url_response).domain\n",
    "        except ConnectionError as e:\n",
    "            err_url = re.search(r'host=\\'([\\w\\-\\.]+)\\'', str(e))\n",
    "            try:\n",
    "                return tldextract.extract(err_url.group(1)).domain\n",
    "            except:\n",
    "                return err_url\n",
    "        except MissingSchema as e:\n",
    "            err_url = 'http://'+ re.search('http://([\\w\\-\\.]+)?', str(e)).group(1)\n",
    "            get_URL_domain(err_url, session)\n",
    "        except:\n",
    "            return url\n",
    "\n",
    "#session = requests.Session()\n",
    "#url1 = tweet_df['URL'].apply(lambda x: get_URL_domain(x, session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_legitimate_df = pd.read_csv('data\\social_honeypot\\legitimate_users.txt', \n",
    "                                  sep = '\\t',\n",
    "                                  names = ['UserID',\n",
    "                                           'CreatedAt',\n",
    "                                           'CollectedAt',\n",
    "                                           'NumberOfFollowings',\n",
    "                                           'NumberOfFollowers',\n",
    "                                           'NumberOfTweets',\n",
    "                                           'LengthOfScreenName',\n",
    "                                           'LengthOfDescriptionInUserPro'])\n",
    "users_polluters_df = pd.read_csv('data/social_honeypot/content_polluters.txt', \n",
    "                                  sep = '\\t',\n",
    "                                  names = ['UserID',\n",
    "                                           'CreatedAt',\n",
    "                                           'CollectedAt',\n",
    "                                           'NumberOfFollowings',\n",
    "                                           'NumberOfFollowers',\n",
    "                                           'NumberOfTweets',\n",
    "                                           'LengthOfScreenName',\n",
    "                                           'LengthOfDescriptionInUserPro'])\n",
    "\n",
    "tweet_df['isLegitimate'] = np.where(tweet_df['UserID'].isin(list(users_legitimate_df['UserID'])), 1, 0)\n",
    "tweet_df['isSpammer'] = np.where(tweet_df['UserID'].isin(list(users_polluters_df['UserID'])), 1, 0)\n",
    "class_label_df = tweet_df[['UserID','IsLegitimate', 'isSpammer']].drop_duplicates(['UserID']).sort_values('UserID').set_index('UserID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Independent variables\"\"\"\n",
    "\n",
    "#Multiview \n",
    "n_v           = 3                            #number of views\n",
    "lambda_v      = np.ones(n_v)                 #regularisation coefficients\n",
    "lambda_star_f = 1\n",
    "lambda_f      = 1\n",
    "beta          = np.array([-np.log(5), np.log(3), 2])         #view weights\n",
    "eta           = 1                            #learning rate\n",
    "K             = 2                            #number of latent features\n",
    "N             = content_view_df.shape[0]     #number of users\n",
    "U=U0          = [None]*n_v\n",
    "V=V0          = [None]*n_v \n",
    "\n",
    "\n",
    "training_set_frac = .80\n",
    "Y = np.array(class_label_df)     #labeled data matrix\n",
    "\n",
    "#SVM\n",
    "alpha = 1\n",
    "W = np.zeros((2,K))\n",
    "lambda_W = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#stack multiple-view feature matrices into list\n",
    "X_nv = [content_view_df.values, URL_view_df.values, hashtag_view_df.values]\n",
    "X_nv = [np.transpose(X_nv[v]) for v in range(n_v)]\n",
    "for v in range(n_v):\n",
    "    num_attr = X_nv[v].shape[0]\n",
    "    U[v]     = np.random.random((num_attr, K))\n",
    "    V[v]     = np.random.random((N, K))\n",
    "    V_star   = np.random.random((N, K))\n",
    "\n",
    "\n",
    "#normalize each view\n",
    "X_nv = [X/scipy.linalg.norm(X, ord = 'fro') for X in X_nv]\n",
    "U_old = U0\n",
    "V_old = V0\n",
    "    \n",
    "    \n",
    "def hinge_loss(z):\n",
    "    if (z <= 0):\n",
    "        return 1/2 - z\n",
    "    elif (z >= 1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/2 * (1 - z)**2\n",
    "    \n",
    "def hinge_loss_derivative(z):\n",
    "    if (z <= 0):\n",
    "        return -z\n",
    "    elif (z >= 1):\n",
    "        return 0\n",
    "    else:\n",
    "        return z - 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36352.632157084714"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_obj_func(beta, U, V, V_star, W, lambda_v, lambda_star_f, lambda_f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DEFINING OBJECTIVE FUNCTION\n",
    "Total Objective Function is O = O_M + O_SVM\n",
    "\"\"\"\n",
    "\n",
    "def total_obj_func(beta, U, V, V_star, W, lambda_v, lambda_star_f, lambda_f):\n",
    "    \"\"\"Calculate Q from U and V\"\"\"\n",
    "    Q = [None]*(n_v)\n",
    "    #Q = np.zeros((n_v, 1))\n",
    "    for v in range(n_v):\n",
    "        diag_vector =  [sum(U[v][:,i]) for i in range(K)]   #i -column index \n",
    "        Q[v]        = np.diag(diag_vector)\n",
    "    \n",
    "    \"\"\"Calculate multiview term O_M of the objective function\"\"\"\n",
    "    term_1      = [X_nv[v] - np.linalg.multi_dot([U[v],\n",
    "                                        np.linalg.inv(Q[v]), \n",
    "                                        Q[v], \n",
    "                                        np.transpose(V[v])]) \n",
    "                   for v in range (n_v)]\n",
    "    term_1_norm = list(map(lambda X: scipy.linalg.norm(X, ord = 'fro')**2, term_1))\n",
    "    term_2      = [V[v].dot(Q[v]) - V_star for v in range (n_v)]\n",
    "    term_2_norm = list(map(lambda X: scipy.linalg.norm(X, ord = 'fro')**2, term_2))  \n",
    "    term_3      = lambda_star_f/2 * np.linalg.norm(V_star, ord = 'fro')\n",
    "    term_4      = [np.linalg.norm(U[v], ord = 'fro')**2 + np.linalg.norm(V[v], ord = 'fro')**2 for v in range (n_v)]\n",
    "    \n",
    "    O_M = 1/2 * np.sum(beta    * term_1_norm +   lambda_v * term_2_norm    ) + lambda_star_f * term_3 +lambda_f/2 * np.sum(term_4)\n",
    "    \n",
    "    \n",
    "    \"\"\"SVM Objective Function Term\"\"\"\n",
    "    l = Y.shape[0]\n",
    "    S = 0\n",
    "    for i in range(l):\n",
    "        S += hinge_loss(Y[i,:].dot(W.dot(np.transpose(V_star[i,:]))))\n",
    "\n",
    "    O_SVM = alpha * S + lambda_W/2 * np.linalg.norm(W, ord = 'fro')\n",
    "    \n",
    "    return O_M + O_SVM\n",
    "\n",
    "    \"\"\"USE\"\"\"\n",
    "#total_obj_func(beta, U, V, V_star, W, lambda_v, lambda_star_f, lambda_f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  1;   Old Value 19588.90280623677; Current Value: 19588.732333802727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 19588.90280623677, 19588.732333802727)"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimize_towards_U_and_V(U, V): \n",
    "    iter_count = 0\n",
    "    max_iter = 1000\n",
    "    func_val_old = 1e100\n",
    "    func_val = total_obj_func(beta, U, V, V_star, W, lambda_v, lambda_star_f, lambda_f) \n",
    "    tolerance = 1e-5\n",
    "               \n",
    "    while (iter_count < max_iter) and (abs(func_val - func_val_old)/func_val > tolerance):\n",
    "        iter_count += 1;\n",
    "        func_val_old = func_val\n",
    "        for v in range(n_v):           \n",
    "            \"\"\"UPDATE U\"\"\"\n",
    "            A = lambda_v[v] * beta[v] * np.transpose(V[v]).dot(V_star)\n",
    "            \"\"\"TODO: Calculate coefficient B\"\"\"               \n",
    "            numerator_U = beta[v]*(X_nv[v].dot(V[v])) \n",
    "            denominator_U = beta[v] * multi_dot([U[v], np.transpose(V[v]), V[v]])\n",
    "            U[v] = U_old[v] * numerator_U/denominator_U\n",
    "\n",
    "            U[v] = U[v]/scipy.linalg.norm(U[v], ord = 'fro')\n",
    "            V[v] = V[v]/scipy.linalg.norm(U[v], ord = 'fro')\n",
    "\n",
    "            \"\"\"UPDATE V\"\"\"\n",
    "            numerator_V = beta[v] * np.transpose(X_nv[v]).dot(U[v]) + lambda_v[v] * beta[v] * V_star\n",
    "            denominator_V = beta[v] * multi_dot([V[v], np.transpose(U[v]), U[v]]) + lambda_v[v] * beta[v] * V[v] + lambda_f * V[v]\n",
    "            V[v] = V_old[v] * numerator_V/denominator_V\n",
    "\n",
    "            \"\"\"Calculate new \"\"\"  \n",
    "            V_old[v] = V[v]\n",
    "            U_old[v] = U[v]\n",
    "            \n",
    "        func_val = total_obj_func(beta, U, V, V_star, W, lambda_v, lambda_star_f, lambda_f)   \n",
    "        print(\"Iter:  {};   Old Value {}; Current Value: {}\".format(iter_count, func_val_old, func_val)) \n",
    "            \n",
    "        \n",
    "    return iter_count, func_val_old, func_val\n",
    "            \n",
    "            \n",
    "optimize_towards_U_and_V(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_towards_V_star_and_W(V_star, W):\n",
    "    iter_count = 0\n",
    "    max_iter = 1000\n",
    "    func_val_old = 1e100\n",
    "    func_val = total_obj_func(beta, U, V, V_star, W, lambda_v, lambda_star_f, lambda_f) \n",
    "    tolerance = 1e-5\n",
    "               \n",
    "    while (iter_count < max_iter) and (abs(func_val - func_val_old)/func_val > tolerance):\n",
    "        iter_count += 1;\n",
    "        func_val_old = func_val\n",
    "        \n",
    "\n",
    "            \n",
    "        func_val = total_obj_func(beta, U, V, V_star, W, lambda_v, lambda_star_f, lambda_f)   \n",
    "        print(\"Iter:  {};   Old Value {}; Current Value: {}\".format(iter_count, func_val_old, func_val)) \n",
    "            \n",
    "        \n",
    "    return iter_count, func_val_old, func_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
